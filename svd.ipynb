{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bTxt1KnCSKY2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@array_function_dispatch(_svd_dispatcher)\n",
      "def svd(a, full_matrices=True, compute_uv=True, hermitian=False):\n",
      "    \"\"\"\n",
      "    Singular Value Decomposition.\n",
      "\n",
      "    When `a` is a 2D array, and ``full_matrices=False``, then it is\n",
      "    factorized as ``u @ np.diag(s) @ vh = (u * s) @ vh``, where\n",
      "    `u` and the Hermitian transpose of `vh` are 2D arrays with\n",
      "    orthonormal columns and `s` is a 1D array of `a`'s singular\n",
      "    values. When `a` is higher-dimensional, SVD is applied in\n",
      "    stacked mode as explained below.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    a : (..., M, N) array_like\n",
      "        A real or complex array with ``a.ndim >= 2``.\n",
      "    full_matrices : bool, optional\n",
      "        If True (default), `u` and `vh` have the shapes ``(..., M, M)`` and\n",
      "        ``(..., N, N)``, respectively.  Otherwise, the shapes are\n",
      "        ``(..., M, K)`` and ``(..., K, N)``, respectively, where\n",
      "        ``K = min(M, N)``.\n",
      "    compute_uv : bool, optional\n",
      "        Whether or not to compute `u` and `vh` in addition to `s`.  True\n",
      "        by default.\n",
      "    hermitian : bool, optional\n",
      "        If True, `a` is assumed to be Hermitian (symmetric if real-valued),\n",
      "        enabling a more efficient method for finding singular values.\n",
      "        Defaults to False.\n",
      "\n",
      "        .. versionadded:: 1.17.0\n",
      "\n",
      "    Returns\n",
      "    -------\n",
      "    When `compute_uv` is True, the result is a namedtuple with the following\n",
      "    attribute names:\n",
      "\n",
      "    U : { (..., M, M), (..., M, K) } array\n",
      "        Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n",
      "        size as those of the input `a`. The size of the last two dimensions\n",
      "        depends on the value of `full_matrices`. Only returned when\n",
      "        `compute_uv` is True.\n",
      "    S : (..., K) array\n",
      "        Vector(s) with the singular values, within each vector sorted in\n",
      "        descending order. The first ``a.ndim - 2`` dimensions have the same\n",
      "        size as those of the input `a`.\n",
      "    Vh : { (..., N, N), (..., K, N) } array\n",
      "        Unitary array(s). The first ``a.ndim - 2`` dimensions have the same\n",
      "        size as those of the input `a`. The size of the last two dimensions\n",
      "        depends on the value of `full_matrices`. Only returned when\n",
      "        `compute_uv` is True.\n",
      "\n",
      "    Raises\n",
      "    ------\n",
      "    LinAlgError\n",
      "        If SVD computation does not converge.\n",
      "\n",
      "    See Also\n",
      "    --------\n",
      "    scipy.linalg.svd : Similar function in SciPy.\n",
      "    scipy.linalg.svdvals : Compute singular values of a matrix.\n",
      "\n",
      "    Notes\n",
      "    -----\n",
      "\n",
      "    .. versionchanged:: 1.8.0\n",
      "       Broadcasting rules apply, see the `numpy.linalg` documentation for\n",
      "       details.\n",
      "\n",
      "    The decomposition is performed using LAPACK routine ``_gesdd``.\n",
      "\n",
      "    SVD is usually described for the factorization of a 2D matrix :math:`A`.\n",
      "    The higher-dimensional case will be discussed below. In the 2D case, SVD is\n",
      "    written as :math:`A = U S V^H`, where :math:`A = a`, :math:`U= u`,\n",
      "    :math:`S= \\\\mathtt{np.diag}(s)` and :math:`V^H = vh`. The 1D array `s`\n",
      "    contains the singular values of `a` and `u` and `vh` are unitary. The rows\n",
      "    of `vh` are the eigenvectors of :math:`A^H A` and the columns of `u` are\n",
      "    the eigenvectors of :math:`A A^H`. In both cases the corresponding\n",
      "    (possibly non-zero) eigenvalues are given by ``s**2``.\n",
      "\n",
      "    If `a` has more than two dimensions, then broadcasting rules apply, as\n",
      "    explained in :ref:`routines.linalg-broadcasting`. This means that SVD is\n",
      "    working in \"stacked\" mode: it iterates over all indices of the first\n",
      "    ``a.ndim - 2`` dimensions and for each combination SVD is applied to the\n",
      "    last two indices. The matrix `a` can be reconstructed from the\n",
      "    decomposition with either ``(u * s[..., None, :]) @ vh`` or\n",
      "    ``u @ (s[..., None] * vh)``. (The ``@`` operator can be replaced by the\n",
      "    function ``np.matmul`` for python versions below 3.5.)\n",
      "\n",
      "    If `a` is a ``matrix`` object (as opposed to an ``ndarray``), then so are\n",
      "    all the return values.\n",
      "\n",
      "    Examples\n",
      "    --------\n",
      "    >>> a = np.random.randn(9, 6) + 1j*np.random.randn(9, 6)\n",
      "    >>> b = np.random.randn(2, 7, 8, 3) + 1j*np.random.randn(2, 7, 8, 3)\n",
      "\n",
      "    Reconstruction based on full SVD, 2D case:\n",
      "\n",
      "    >>> U, S, Vh = np.linalg.svd(a, full_matrices=True)\n",
      "    >>> U.shape, S.shape, Vh.shape\n",
      "    ((9, 9), (6,), (6, 6))\n",
      "    >>> np.allclose(a, np.dot(U[:, :6] * S, Vh))\n",
      "    True\n",
      "    >>> smat = np.zeros((9, 6), dtype=complex)\n",
      "    >>> smat[:6, :6] = np.diag(S)\n",
      "    >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\n",
      "    True\n",
      "\n",
      "    Reconstruction based on reduced SVD, 2D case:\n",
      "\n",
      "    >>> U, S, Vh = np.linalg.svd(a, full_matrices=False)\n",
      "    >>> U.shape, S.shape, Vh.shape\n",
      "    ((9, 6), (6,), (6, 6))\n",
      "    >>> np.allclose(a, np.dot(U * S, Vh))\n",
      "    True\n",
      "    >>> smat = np.diag(S)\n",
      "    >>> np.allclose(a, np.dot(U, np.dot(smat, Vh)))\n",
      "    True\n",
      "\n",
      "    Reconstruction based on full SVD, 4D case:\n",
      "\n",
      "    >>> U, S, Vh = np.linalg.svd(b, full_matrices=True)\n",
      "    >>> U.shape, S.shape, Vh.shape\n",
      "    ((2, 7, 8, 8), (2, 7, 3), (2, 7, 3, 3))\n",
      "    >>> np.allclose(b, np.matmul(U[..., :3] * S[..., None, :], Vh))\n",
      "    True\n",
      "    >>> np.allclose(b, np.matmul(U[..., :3], S[..., None] * Vh))\n",
      "    True\n",
      "\n",
      "    Reconstruction based on reduced SVD, 4D case:\n",
      "\n",
      "    >>> U, S, Vh = np.linalg.svd(b, full_matrices=False)\n",
      "    >>> U.shape, S.shape, Vh.shape\n",
      "    ((2, 7, 8, 3), (2, 7, 3), (2, 7, 3, 3))\n",
      "    >>> np.allclose(b, np.matmul(U * S[..., None, :], Vh))\n",
      "    True\n",
      "    >>> np.allclose(b, np.matmul(U, S[..., None] * Vh))\n",
      "    True\n",
      "\n",
      "    \"\"\"\n",
      "    import numpy as _nx\n",
      "    a, wrap = _makearray(a)\n",
      "\n",
      "    if hermitian:\n",
      "        # note: lapack svd returns eigenvalues with s ** 2 sorted descending,\n",
      "        # but eig returns s sorted ascending, so we re-order the eigenvalues\n",
      "        # and related arrays to have the correct order\n",
      "        if compute_uv:\n",
      "            s, u = eigh(a)\n",
      "            sgn = sign(s)\n",
      "            s = abs(s)\n",
      "            sidx = argsort(s)[..., ::-1]\n",
      "            sgn = _nx.take_along_axis(sgn, sidx, axis=-1)\n",
      "            s = _nx.take_along_axis(s, sidx, axis=-1)\n",
      "            u = _nx.take_along_axis(u, sidx[..., None, :], axis=-1)\n",
      "            # singular values are unsigned, move the sign into v\n",
      "            vt = transpose(u * sgn[..., None, :]).conjugate()\n",
      "            return SVDResult(wrap(u), s, wrap(vt))\n",
      "        else:\n",
      "            s = eigvalsh(a)\n",
      "            s = abs(s)\n",
      "            return sort(s)[..., ::-1]\n",
      "\n",
      "    _assert_stacked_2d(a)\n",
      "    t, result_t = _commonType(a)\n",
      "\n",
      "    extobj = get_linalg_error_extobj(_raise_linalgerror_svd_nonconvergence)\n",
      "\n",
      "    m, n = a.shape[-2:]\n",
      "    if compute_uv:\n",
      "        if full_matrices:\n",
      "            if m < n:\n",
      "                gufunc = _umath_linalg.svd_m_f\n",
      "            else:\n",
      "                gufunc = _umath_linalg.svd_n_f\n",
      "        else:\n",
      "            if m < n:\n",
      "                gufunc = _umath_linalg.svd_m_s\n",
      "            else:\n",
      "                gufunc = _umath_linalg.svd_n_s\n",
      "\n",
      "        signature = 'D->DdD' if isComplexType(t) else 'd->ddd'\n",
      "        u, s, vh = gufunc(a, signature=signature, extobj=extobj)\n",
      "        u = u.astype(result_t, copy=False)\n",
      "        s = s.astype(_realType(result_t), copy=False)\n",
      "        vh = vh.astype(result_t, copy=False)\n",
      "        return SVDResult(wrap(u), s, wrap(vh))\n",
      "    else:\n",
      "        if m < n:\n",
      "            gufunc = _umath_linalg.svd_m\n",
      "        else:\n",
      "            gufunc = _umath_linalg.svd_n\n",
      "\n",
      "        signature = 'D->d' if isComplexType(t) else 'd->d'\n",
      "        s = gufunc(a, signature=signature, extobj=extobj)\n",
      "        s = s.astype(_realType(result_t), copy=False)\n",
      "        return s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "# from scipy import linalg\n",
    "import numpy as np\n",
    "print(inspect.getsource(np.linalg.svd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C_tU9R-Uuixu",
    "outputId": "c68b675a-508c-4203-d58b-cca5d96c7dd6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#  Download and load CIFAR-10 dataset\n",
    "transform = transforms.Compose([transforms.Grayscale(num_output_channels=1), transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
    "\n",
    "def compute_svd_singular_values(image_batch):\n",
    "    singular_values = []\n",
    "    for image in image_batch:\n",
    "        squeeze = image.squeeze()\n",
    "        U, S, V = torch.svd(squeeze)\n",
    "        singular_values.append(S)\n",
    "    return torch.stack(singular_values)\n",
    "\n",
    "# Compute singular values for train and test sets\n",
    "train_singular_values = [compute_svd_singular_values(batch[0]).to(device) for batch in trainloader]\n",
    "test_singular_values = [compute_svd_singular_values(batch[0]).to(device) for batch in testloader]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SFxrnA5xvrGG",
    "outputId": "27ecdd24-0003-4cd4-c745-235ce20e30ec"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_singular_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZCrr0_4yB9w"
   },
   "source": [
    "### Step 2: Neural Network Training\n",
    "\n",
    "#### A. Define the Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KF4aLf03xeFd"
   },
   "outputs": [],
   "source": [
    "class SVDPredictor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVDPredictor, self).__init__()\n",
    "        self.fc1 = nn.Linear(32*32, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 32)  # Predicting 32 singular values\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.5)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# move the model to gpu\n",
    "model = SVDPredictor().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZAjS1C4yLof"
   },
   "source": [
    "#### B. Define Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "F_bcKTXEyQy2"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jAFgEBUyicy"
   },
   "source": [
    "#### C. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFIyGBHywHiy",
    "outputId": "12a3f109-a73d-41fb-e67f-5b053c2561bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Step [100/782], Loss: 0.8692\n",
      "Epoch [1/50], Step [200/782], Loss: 0.7920\n",
      "Epoch [1/50], Step [300/782], Loss: 0.7587\n",
      "Epoch [1/50], Step [400/782], Loss: 0.7413\n",
      "Epoch [1/50], Step [500/782], Loss: 0.7311\n",
      "Epoch [1/50], Step [600/782], Loss: 0.7219\n",
      "Epoch [1/50], Step [700/782], Loss: 0.7093\n",
      "Epoch [2/50], Step [100/782], Loss: 0.6949\n",
      "Epoch [2/50], Step [200/782], Loss: 0.6863\n",
      "Epoch [2/50], Step [300/782], Loss: 0.6813\n",
      "Epoch [2/50], Step [400/782], Loss: 0.6758\n",
      "Epoch [2/50], Step [500/782], Loss: 0.6714\n",
      "Epoch [2/50], Step [600/782], Loss: 0.6666\n",
      "Epoch [2/50], Step [700/782], Loss: 0.6562\n",
      "Epoch [3/50], Step [100/782], Loss: 0.6487\n",
      "Epoch [3/50], Step [200/782], Loss: 0.6383\n",
      "Epoch [3/50], Step [300/782], Loss: 0.6356\n",
      "Epoch [3/50], Step [400/782], Loss: 0.6271\n",
      "Epoch [3/50], Step [500/782], Loss: 0.6248\n",
      "Epoch [3/50], Step [600/782], Loss: 0.6190\n",
      "Epoch [3/50], Step [700/782], Loss: 0.6069\n",
      "Epoch [4/50], Step [100/782], Loss: 0.5945\n",
      "Epoch [4/50], Step [200/782], Loss: 0.5842\n",
      "Epoch [4/50], Step [300/782], Loss: 0.5773\n",
      "Epoch [4/50], Step [400/782], Loss: 0.5643\n",
      "Epoch [4/50], Step [500/782], Loss: 0.5590\n",
      "Epoch [4/50], Step [600/782], Loss: 0.5472\n",
      "Epoch [4/50], Step [700/782], Loss: 0.5329\n",
      "Epoch [5/50], Step [100/782], Loss: 0.5109\n",
      "Epoch [5/50], Step [200/782], Loss: 0.4992\n",
      "Epoch [5/50], Step [300/782], Loss: 0.4942\n",
      "Epoch [5/50], Step [400/782], Loss: 0.4828\n",
      "Epoch [5/50], Step [500/782], Loss: 0.4759\n",
      "Epoch [5/50], Step [600/782], Loss: 0.4713\n",
      "Epoch [5/50], Step [700/782], Loss: 0.4598\n",
      "Epoch [6/50], Step [100/782], Loss: 0.4439\n",
      "Epoch [6/50], Step [200/782], Loss: 0.4355\n",
      "Epoch [6/50], Step [300/782], Loss: 0.4308\n",
      "Epoch [6/50], Step [400/782], Loss: 0.4237\n",
      "Epoch [6/50], Step [500/782], Loss: 0.4203\n",
      "Epoch [6/50], Step [600/782], Loss: 0.4126\n",
      "Epoch [6/50], Step [700/782], Loss: 0.4037\n",
      "Epoch [7/50], Step [100/782], Loss: 0.3924\n",
      "Epoch [7/50], Step [200/782], Loss: 0.3865\n",
      "Epoch [7/50], Step [300/782], Loss: 0.3790\n",
      "Epoch [7/50], Step [400/782], Loss: 0.3731\n",
      "Epoch [7/50], Step [500/782], Loss: 0.3722\n",
      "Epoch [7/50], Step [600/782], Loss: 0.3662\n",
      "Epoch [7/50], Step [700/782], Loss: 0.3589\n",
      "Epoch [8/50], Step [100/782], Loss: 0.3509\n",
      "Epoch [8/50], Step [200/782], Loss: 0.3411\n",
      "Epoch [8/50], Step [300/782], Loss: 0.3393\n",
      "Epoch [8/50], Step [400/782], Loss: 0.3342\n",
      "Epoch [8/50], Step [500/782], Loss: 0.3323\n",
      "Epoch [8/50], Step [600/782], Loss: 0.3287\n",
      "Epoch [8/50], Step [700/782], Loss: 0.3224\n",
      "Epoch [9/50], Step [100/782], Loss: 0.3173\n",
      "Epoch [9/50], Step [200/782], Loss: 0.3102\n",
      "Epoch [9/50], Step [300/782], Loss: 0.3083\n",
      "Epoch [9/50], Step [400/782], Loss: 0.3070\n",
      "Epoch [9/50], Step [500/782], Loss: 0.3078\n",
      "Epoch [9/50], Step [600/782], Loss: 0.3075\n",
      "Epoch [9/50], Step [700/782], Loss: 0.3014\n",
      "Epoch [10/50], Step [100/782], Loss: 0.2956\n",
      "Epoch [10/50], Step [200/782], Loss: 0.2928\n",
      "Epoch [10/50], Step [300/782], Loss: 0.2933\n",
      "Epoch [10/50], Step [400/782], Loss: 0.2891\n",
      "Epoch [10/50], Step [500/782], Loss: 0.2901\n",
      "Epoch [10/50], Step [600/782], Loss: 0.2884\n",
      "Epoch [10/50], Step [700/782], Loss: 0.2875\n",
      "Epoch [11/50], Step [100/782], Loss: 0.2820\n",
      "Epoch [11/50], Step [200/782], Loss: 0.2802\n",
      "Epoch [11/50], Step [300/782], Loss: 0.2851\n",
      "Epoch [11/50], Step [400/782], Loss: 0.2787\n",
      "Epoch [11/50], Step [500/782], Loss: 0.2797\n",
      "Epoch [11/50], Step [600/782], Loss: 0.2794\n",
      "Epoch [11/50], Step [700/782], Loss: 0.2762\n",
      "Epoch [12/50], Step [100/782], Loss: 0.2761\n",
      "Epoch [12/50], Step [200/782], Loss: 0.2715\n",
      "Epoch [12/50], Step [300/782], Loss: 0.2747\n",
      "Epoch [12/50], Step [400/782], Loss: 0.2719\n",
      "Epoch [12/50], Step [500/782], Loss: 0.2754\n",
      "Epoch [12/50], Step [600/782], Loss: 0.2765\n",
      "Epoch [12/50], Step [700/782], Loss: 0.2711\n",
      "Epoch [13/50], Step [100/782], Loss: 0.2729\n",
      "Epoch [13/50], Step [200/782], Loss: 0.2654\n",
      "Epoch [13/50], Step [300/782], Loss: 0.2709\n",
      "Epoch [13/50], Step [400/782], Loss: 0.2681\n",
      "Epoch [13/50], Step [500/782], Loss: 0.2682\n",
      "Epoch [13/50], Step [600/782], Loss: 0.2664\n",
      "Epoch [13/50], Step [700/782], Loss: 0.2656\n",
      "Epoch [14/50], Step [100/782], Loss: 0.2656\n",
      "Epoch [14/50], Step [200/782], Loss: 0.2630\n",
      "Epoch [14/50], Step [300/782], Loss: 0.2659\n",
      "Epoch [14/50], Step [400/782], Loss: 0.2645\n",
      "Epoch [14/50], Step [500/782], Loss: 0.2664\n",
      "Epoch [14/50], Step [600/782], Loss: 0.2626\n",
      "Epoch [14/50], Step [700/782], Loss: 0.2614\n",
      "Epoch [15/50], Step [100/782], Loss: 0.2608\n",
      "Epoch [15/50], Step [200/782], Loss: 0.2569\n",
      "Epoch [15/50], Step [300/782], Loss: 0.2593\n",
      "Epoch [15/50], Step [400/782], Loss: 0.2593\n",
      "Epoch [15/50], Step [500/782], Loss: 0.2612\n",
      "Epoch [15/50], Step [600/782], Loss: 0.2598\n",
      "Epoch [15/50], Step [700/782], Loss: 0.2609\n",
      "Epoch [16/50], Step [100/782], Loss: 0.2598\n",
      "Epoch [16/50], Step [200/782], Loss: 0.2568\n",
      "Epoch [16/50], Step [300/782], Loss: 0.2580\n",
      "Epoch [16/50], Step [400/782], Loss: 0.2583\n",
      "Epoch [16/50], Step [500/782], Loss: 0.2565\n",
      "Epoch [16/50], Step [600/782], Loss: 0.2576\n",
      "Epoch [16/50], Step [700/782], Loss: 0.2561\n",
      "Epoch [17/50], Step [100/782], Loss: 0.2578\n",
      "Epoch [17/50], Step [200/782], Loss: 0.2515\n",
      "Epoch [17/50], Step [300/782], Loss: 0.2536\n",
      "Epoch [17/50], Step [400/782], Loss: 0.2526\n",
      "Epoch [17/50], Step [500/782], Loss: 0.2561\n",
      "Epoch [17/50], Step [600/782], Loss: 0.2552\n",
      "Epoch [17/50], Step [700/782], Loss: 0.2526\n",
      "Epoch [18/50], Step [100/782], Loss: 0.2515\n",
      "Epoch [18/50], Step [200/782], Loss: 0.2505\n",
      "Epoch [18/50], Step [300/782], Loss: 0.2528\n",
      "Epoch [18/50], Step [400/782], Loss: 0.2524\n",
      "Epoch [18/50], Step [500/782], Loss: 0.2542\n",
      "Epoch [18/50], Step [600/782], Loss: 0.2534\n",
      "Epoch [18/50], Step [700/782], Loss: 0.2491\n",
      "Epoch [19/50], Step [100/782], Loss: 0.2490\n",
      "Epoch [19/50], Step [200/782], Loss: 0.2496\n",
      "Epoch [19/50], Step [300/782], Loss: 0.2488\n",
      "Epoch [19/50], Step [400/782], Loss: 0.2492\n",
      "Epoch [19/50], Step [500/782], Loss: 0.2536\n",
      "Epoch [19/50], Step [600/782], Loss: 0.2454\n",
      "Epoch [19/50], Step [700/782], Loss: 0.2459\n",
      "Epoch [20/50], Step [100/782], Loss: 0.2457\n",
      "Epoch [20/50], Step [200/782], Loss: 0.2464\n",
      "Epoch [20/50], Step [300/782], Loss: 0.2451\n",
      "Epoch [20/50], Step [400/782], Loss: 0.2474\n",
      "Epoch [20/50], Step [500/782], Loss: 0.2490\n",
      "Epoch [20/50], Step [600/782], Loss: 0.2468\n",
      "Epoch [20/50], Step [700/782], Loss: 0.2462\n",
      "Epoch [21/50], Step [100/782], Loss: 0.2453\n",
      "Epoch [21/50], Step [200/782], Loss: 0.2425\n",
      "Epoch [21/50], Step [300/782], Loss: 0.2437\n",
      "Epoch [21/50], Step [400/782], Loss: 0.2422\n",
      "Epoch [21/50], Step [500/782], Loss: 0.2472\n",
      "Epoch [21/50], Step [600/782], Loss: 0.2459\n",
      "Epoch [21/50], Step [700/782], Loss: 0.2443\n",
      "Epoch [22/50], Step [100/782], Loss: 0.2421\n",
      "Epoch [22/50], Step [200/782], Loss: 0.2427\n",
      "Epoch [22/50], Step [300/782], Loss: 0.2419\n",
      "Epoch [22/50], Step [400/782], Loss: 0.2405\n",
      "Epoch [22/50], Step [500/782], Loss: 0.2459\n",
      "Epoch [22/50], Step [600/782], Loss: 0.2440\n",
      "Epoch [22/50], Step [700/782], Loss: 0.2392\n",
      "Epoch [23/50], Step [100/782], Loss: 0.2422\n",
      "Epoch [23/50], Step [200/782], Loss: 0.2382\n",
      "Epoch [23/50], Step [300/782], Loss: 0.2402\n",
      "Epoch [23/50], Step [400/782], Loss: 0.2426\n",
      "Epoch [23/50], Step [500/782], Loss: 0.2442\n",
      "Epoch [23/50], Step [600/782], Loss: 0.2428\n",
      "Epoch [23/50], Step [700/782], Loss: 0.2391\n",
      "Epoch [24/50], Step [100/782], Loss: 0.2389\n",
      "Epoch [24/50], Step [200/782], Loss: 0.2388\n",
      "Epoch [24/50], Step [300/782], Loss: 0.2381\n",
      "Epoch [24/50], Step [400/782], Loss: 0.2376\n",
      "Epoch [24/50], Step [500/782], Loss: 0.2395\n",
      "Epoch [24/50], Step [600/782], Loss: 0.2389\n",
      "Epoch [24/50], Step [700/782], Loss: 0.2393\n",
      "Epoch [25/50], Step [100/782], Loss: 0.2391\n",
      "Epoch [25/50], Step [200/782], Loss: 0.2337\n",
      "Epoch [25/50], Step [300/782], Loss: 0.2364\n",
      "Epoch [25/50], Step [400/782], Loss: 0.2381\n",
      "Epoch [25/50], Step [500/782], Loss: 0.2403\n",
      "Epoch [25/50], Step [600/782], Loss: 0.2397\n",
      "Epoch [25/50], Step [700/782], Loss: 0.2358\n",
      "Epoch [26/50], Step [100/782], Loss: 0.2371\n",
      "Epoch [26/50], Step [200/782], Loss: 0.2349\n",
      "Epoch [26/50], Step [300/782], Loss: 0.2347\n",
      "Epoch [26/50], Step [400/782], Loss: 0.2344\n",
      "Epoch [26/50], Step [500/782], Loss: 0.2390\n",
      "Epoch [26/50], Step [600/782], Loss: 0.2350\n",
      "Epoch [26/50], Step [700/782], Loss: 0.2350\n",
      "Epoch [27/50], Step [100/782], Loss: 0.2344\n",
      "Epoch [27/50], Step [200/782], Loss: 0.2303\n",
      "Epoch [27/50], Step [300/782], Loss: 0.2346\n",
      "Epoch [27/50], Step [400/782], Loss: 0.2330\n",
      "Epoch [27/50], Step [500/782], Loss: 0.2379\n",
      "Epoch [27/50], Step [600/782], Loss: 0.2348\n",
      "Epoch [27/50], Step [700/782], Loss: 0.2335\n",
      "Epoch [28/50], Step [100/782], Loss: 0.2354\n",
      "Epoch [28/50], Step [200/782], Loss: 0.2315\n",
      "Epoch [28/50], Step [300/782], Loss: 0.2339\n",
      "Epoch [28/50], Step [400/782], Loss: 0.2311\n",
      "Epoch [28/50], Step [500/782], Loss: 0.2347\n",
      "Epoch [28/50], Step [600/782], Loss: 0.2337\n",
      "Epoch [28/50], Step [700/782], Loss: 0.2334\n",
      "Epoch [29/50], Step [100/782], Loss: 0.2323\n",
      "Epoch [29/50], Step [200/782], Loss: 0.2293\n",
      "Epoch [29/50], Step [300/782], Loss: 0.2314\n",
      "Epoch [29/50], Step [400/782], Loss: 0.2326\n",
      "Epoch [29/50], Step [500/782], Loss: 0.2341\n",
      "Epoch [29/50], Step [600/782], Loss: 0.2344\n",
      "Epoch [29/50], Step [700/782], Loss: 0.2311\n",
      "Epoch [30/50], Step [100/782], Loss: 0.2297\n",
      "Epoch [30/50], Step [200/782], Loss: 0.2292\n",
      "Epoch [30/50], Step [300/782], Loss: 0.2299\n",
      "Epoch [30/50], Step [400/782], Loss: 0.2314\n",
      "Epoch [30/50], Step [500/782], Loss: 0.2330\n",
      "Epoch [30/50], Step [600/782], Loss: 0.2315\n",
      "Epoch [30/50], Step [700/782], Loss: 0.2324\n",
      "Epoch [31/50], Step [100/782], Loss: 0.2296\n",
      "Epoch [31/50], Step [200/782], Loss: 0.2281\n",
      "Epoch [31/50], Step [300/782], Loss: 0.2298\n",
      "Epoch [31/50], Step [400/782], Loss: 0.2291\n",
      "Epoch [31/50], Step [500/782], Loss: 0.2336\n",
      "Epoch [31/50], Step [600/782], Loss: 0.2310\n",
      "Epoch [31/50], Step [700/782], Loss: 0.2286\n",
      "Epoch [32/50], Step [100/782], Loss: 0.2279\n",
      "Epoch [32/50], Step [200/782], Loss: 0.2258\n",
      "Epoch [32/50], Step [300/782], Loss: 0.2279\n",
      "Epoch [32/50], Step [400/782], Loss: 0.2277\n",
      "Epoch [32/50], Step [500/782], Loss: 0.2319\n",
      "Epoch [32/50], Step [600/782], Loss: 0.2289\n",
      "Epoch [32/50], Step [700/782], Loss: 0.2277\n",
      "Epoch [33/50], Step [100/782], Loss: 0.2280\n",
      "Epoch [33/50], Step [200/782], Loss: 0.2247\n",
      "Epoch [33/50], Step [300/782], Loss: 0.2293\n",
      "Epoch [33/50], Step [400/782], Loss: 0.2276\n",
      "Epoch [33/50], Step [500/782], Loss: 0.2303\n",
      "Epoch [33/50], Step [600/782], Loss: 0.2281\n",
      "Epoch [33/50], Step [700/782], Loss: 0.2283\n",
      "Epoch [34/50], Step [100/782], Loss: 0.2252\n",
      "Epoch [34/50], Step [200/782], Loss: 0.2264\n",
      "Epoch [34/50], Step [300/782], Loss: 0.2261\n",
      "Epoch [34/50], Step [400/782], Loss: 0.2269\n",
      "Epoch [34/50], Step [500/782], Loss: 0.2303\n",
      "Epoch [34/50], Step [600/782], Loss: 0.2279\n",
      "Epoch [34/50], Step [700/782], Loss: 0.2264\n",
      "Epoch [35/50], Step [100/782], Loss: 0.2280\n",
      "Epoch [35/50], Step [200/782], Loss: 0.2229\n",
      "Epoch [35/50], Step [300/782], Loss: 0.2267\n",
      "Epoch [35/50], Step [400/782], Loss: 0.2269\n",
      "Epoch [35/50], Step [500/782], Loss: 0.2297\n",
      "Epoch [35/50], Step [600/782], Loss: 0.2273\n",
      "Epoch [35/50], Step [700/782], Loss: 0.2262\n",
      "Epoch [36/50], Step [100/782], Loss: 0.2251\n",
      "Epoch [36/50], Step [200/782], Loss: 0.2232\n",
      "Epoch [36/50], Step [300/782], Loss: 0.2283\n",
      "Epoch [36/50], Step [400/782], Loss: 0.2259\n",
      "Epoch [36/50], Step [500/782], Loss: 0.2307\n",
      "Epoch [36/50], Step [600/782], Loss: 0.2277\n",
      "Epoch [36/50], Step [700/782], Loss: 0.2256\n",
      "Epoch [37/50], Step [100/782], Loss: 0.2256\n",
      "Epoch [37/50], Step [200/782], Loss: 0.2227\n",
      "Epoch [37/50], Step [300/782], Loss: 0.2258\n",
      "Epoch [37/50], Step [400/782], Loss: 0.2241\n",
      "Epoch [37/50], Step [500/782], Loss: 0.2304\n",
      "Epoch [37/50], Step [600/782], Loss: 0.2271\n",
      "Epoch [37/50], Step [700/782], Loss: 0.2265\n",
      "Epoch [38/50], Step [100/782], Loss: 0.2254\n",
      "Epoch [38/50], Step [200/782], Loss: 0.2212\n",
      "Epoch [38/50], Step [300/782], Loss: 0.2254\n",
      "Epoch [38/50], Step [400/782], Loss: 0.2241\n",
      "Epoch [38/50], Step [500/782], Loss: 0.2282\n",
      "Epoch [38/50], Step [600/782], Loss: 0.2286\n",
      "Epoch [38/50], Step [700/782], Loss: 0.2240\n",
      "Epoch [39/50], Step [100/782], Loss: 0.2240\n",
      "Epoch [39/50], Step [200/782], Loss: 0.2222\n",
      "Epoch [39/50], Step [300/782], Loss: 0.2252\n",
      "Epoch [39/50], Step [400/782], Loss: 0.2245\n",
      "Epoch [39/50], Step [500/782], Loss: 0.2271\n",
      "Epoch [39/50], Step [600/782], Loss: 0.2256\n",
      "Epoch [39/50], Step [700/782], Loss: 0.2250\n",
      "Epoch [40/50], Step [100/782], Loss: 0.2247\n",
      "Epoch [40/50], Step [200/782], Loss: 0.2224\n",
      "Epoch [40/50], Step [300/782], Loss: 0.2242\n",
      "Epoch [40/50], Step [400/782], Loss: 0.2228\n",
      "Epoch [40/50], Step [500/782], Loss: 0.2283\n",
      "Epoch [40/50], Step [600/782], Loss: 0.2251\n",
      "Epoch [40/50], Step [700/782], Loss: 0.2235\n",
      "Epoch [41/50], Step [100/782], Loss: 0.2225\n",
      "Epoch [41/50], Step [200/782], Loss: 0.2215\n",
      "Epoch [41/50], Step [300/782], Loss: 0.2247\n",
      "Epoch [41/50], Step [400/782], Loss: 0.2250\n",
      "Epoch [41/50], Step [500/782], Loss: 0.2267\n",
      "Epoch [41/50], Step [600/782], Loss: 0.2252\n",
      "Epoch [41/50], Step [700/782], Loss: 0.2238\n",
      "Epoch [42/50], Step [100/782], Loss: 0.2237\n",
      "Epoch [42/50], Step [200/782], Loss: 0.2224\n",
      "Epoch [42/50], Step [300/782], Loss: 0.2250\n",
      "Epoch [42/50], Step [400/782], Loss: 0.2258\n",
      "Epoch [42/50], Step [500/782], Loss: 0.2276\n",
      "Epoch [42/50], Step [600/782], Loss: 0.2265\n",
      "Epoch [42/50], Step [700/782], Loss: 0.2239\n",
      "Epoch [43/50], Step [100/782], Loss: 0.2246\n",
      "Epoch [43/50], Step [200/782], Loss: 0.2209\n",
      "Epoch [43/50], Step [300/782], Loss: 0.2238\n",
      "Epoch [43/50], Step [400/782], Loss: 0.2234\n",
      "Epoch [43/50], Step [500/782], Loss: 0.2268\n",
      "Epoch [43/50], Step [600/782], Loss: 0.2253\n",
      "Epoch [43/50], Step [700/782], Loss: 0.2227\n",
      "Epoch [44/50], Step [100/782], Loss: 0.2225\n",
      "Epoch [44/50], Step [200/782], Loss: 0.2220\n",
      "Epoch [44/50], Step [300/782], Loss: 0.2243\n",
      "Epoch [44/50], Step [400/782], Loss: 0.2234\n",
      "Epoch [44/50], Step [500/782], Loss: 0.2268\n",
      "Epoch [44/50], Step [600/782], Loss: 0.2244\n",
      "Epoch [44/50], Step [700/782], Loss: 0.2244\n",
      "Epoch [45/50], Step [100/782], Loss: 0.2250\n",
      "Epoch [45/50], Step [200/782], Loss: 0.2211\n",
      "Epoch [45/50], Step [300/782], Loss: 0.2236\n",
      "Epoch [45/50], Step [400/782], Loss: 0.2248\n",
      "Epoch [45/50], Step [500/782], Loss: 0.2283\n",
      "Epoch [45/50], Step [600/782], Loss: 0.2229\n",
      "Epoch [45/50], Step [700/782], Loss: 0.2237\n",
      "Epoch [46/50], Step [100/782], Loss: 0.2238\n",
      "Epoch [46/50], Step [200/782], Loss: 0.2221\n",
      "Epoch [46/50], Step [300/782], Loss: 0.2244\n",
      "Epoch [46/50], Step [400/782], Loss: 0.2238\n",
      "Epoch [46/50], Step [500/782], Loss: 0.2265\n",
      "Epoch [46/50], Step [600/782], Loss: 0.2254\n",
      "Epoch [46/50], Step [700/782], Loss: 0.2219\n",
      "Epoch [47/50], Step [100/782], Loss: 0.2220\n",
      "Epoch [47/50], Step [200/782], Loss: 0.2206\n",
      "Epoch [47/50], Step [300/782], Loss: 0.2245\n",
      "Epoch [47/50], Step [400/782], Loss: 0.2233\n",
      "Epoch [47/50], Step [500/782], Loss: 0.2257\n",
      "Epoch [47/50], Step [600/782], Loss: 0.2241\n",
      "Epoch [47/50], Step [700/782], Loss: 0.2230\n",
      "Epoch [48/50], Step [100/782], Loss: 0.2234\n",
      "Epoch [48/50], Step [200/782], Loss: 0.2208\n",
      "Epoch [48/50], Step [300/782], Loss: 0.2213\n",
      "Epoch [48/50], Step [400/782], Loss: 0.2230\n",
      "Epoch [48/50], Step [500/782], Loss: 0.2263\n",
      "Epoch [48/50], Step [600/782], Loss: 0.2240\n",
      "Epoch [48/50], Step [700/782], Loss: 0.2233\n",
      "Epoch [49/50], Step [100/782], Loss: 0.2220\n",
      "Epoch [49/50], Step [200/782], Loss: 0.2211\n",
      "Epoch [49/50], Step [300/782], Loss: 0.2245\n",
      "Epoch [49/50], Step [400/782], Loss: 0.2220\n",
      "Epoch [49/50], Step [500/782], Loss: 0.2262\n",
      "Epoch [49/50], Step [600/782], Loss: 0.2228\n",
      "Epoch [49/50], Step [700/782], Loss: 0.2246\n",
      "Epoch [50/50], Step [100/782], Loss: 0.2218\n",
      "Epoch [50/50], Step [200/782], Loss: 0.2192\n",
      "Epoch [50/50], Step [300/782], Loss: 0.2240\n",
      "Epoch [50/50], Step [400/782], Loss: 0.2243\n",
      "Epoch [50/50], Step [500/782], Loss: 0.2265\n",
      "Epoch [50/50], Step [600/782], Loss: 0.2228\n",
      "Epoch [50/50], Step [700/782], Loss: 0.2217\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 50  # Set the number of epochs\n",
    "print_every_n_batches = 100  # Print log after every 100 batches\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, _) in enumerate(trainloader):\n",
    "        images = images.view(images.shape[0], -1).to(device)\n",
    "        labels = train_singular_values[i]  # Precomputed singular values\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Log the running loss\n",
    "        if (i + 1) % print_every_n_batches == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(trainloader)}], Loss: {running_loss / print_every_n_batches:.4f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UFewTIJh0jKv"
   },
   "source": [
    "### Step 3: Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTH8TYUd1R2b"
   },
   "source": [
    "##### Implementing NMSE and MAE Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ehXV5nET1J6F"
   },
   "outputs": [],
   "source": [
    "def compute_nmse(outputs, labels):\n",
    "    mse = ((outputs - labels) ** 2).mean(axis=1)\n",
    "    norm = (labels ** 2).mean(axis=1)\n",
    "    nmse = mse / norm\n",
    "    return nmse\n",
    "\n",
    "def compute_mae(outputs, labels):\n",
    "    mae = torch.abs(outputs - labels).mean(axis=1)\n",
    "    return mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpsMur4-yvuf",
    "outputId": "95bab850-406c-49bd-bcfb-8c6189b5adfc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average NMSE: 0.0876\n",
      "Average MAE: 0.2137\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "total_nmse = []\n",
    "total_mae = []\n",
    "total_samples = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, _) in enumerate(testloader):\n",
    "        images = images.view(images.shape[0], -1).to(device)\n",
    "        labels = test_singular_values[i]  # Use precomputed singular values\n",
    "\n",
    "        outputs = model(images)\n",
    "        nmse = compute_nmse(outputs, labels)\n",
    "        mae = compute_mae(outputs, labels)\n",
    "\n",
    "        total_nmse.extend(nmse.tolist())\n",
    "        total_mae.extend(mae.tolist())\n",
    "        total_samples += images.size(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Q14ruxM1yk6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
